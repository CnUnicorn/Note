

# 贷款年化利率

利率和坏账损失统一变换口径，比如都是年化的



贷款年化利率：

APR（名义利率）：

![image-20241022203848549](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20241022203848549.png)

IRR（真实利率）：

**IRR原始公式：**

**每一期的还款金额都折算到放宽日同一时间点进行计算：**针对12期的产品，最后一期支付的还款金额，被占用了12个月；倒数第二期支付的还款金额，被占用了11个月；···依次类推，第一期支付的还款金额，被占用了1个月。

粗浅的理解：相当于被占用的金额，按照利息贬值，越晚还款的前贬值越多，最后折现成本金。

![image-20241022205242433](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20241022205242433.png)

公式介绍：

**n：年内还款频率。**

**T：还款年数。**

**如一年期限的产品，按月还款：n=12，T=1**

**五年期产品，按月还款：n=12，T=5**

![image-20241022205926211](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20241022205926211.png)



![image-20241022215639310](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20241022215639310.png)

**从不同角度考虑IRR：**

![image-20241022215719399](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20241022215719399.png)





**从本金占用的角度考虑的IRR公式：**

（每期归还的利息 / 每期平均占用的本金）*一年周转次数

![image-20241022212443137](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20241022212443137.png)

![image-20241022215813435](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20241022215813435.png)





**从满额占用本金时间的角度考虑的IRR公式：**

业内也叫**生息月份**或者**生息期数**，也是**久期**的概念，只不过叫法不同：

![image-20241022212941753](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20241022212941753.png)

![image-20241022215838341](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20241022215838341.png)



**APR和IRR的区别：**

从公式的角度看，APR与IRR的两种公式的形式都是一样的

APR与从占用平均本金的IRR公式对比：差别在，分母变成了平均占用本金

APR与从占用满额本金时间的IRR公式对比：差别在，贷款期限（月）变成了，占用满额本金的月份。



对于相同的还款利息，相同的还款本金：计算得到的APR一定是比IRR要小的。

从本金角度考虑：APR（名义利率）认为每一期都占用了满额的本金；而IRR考虑到随着还款进行，占用本金变少，所以相同利息下，IRR要大于APR。

比如三期的网贷：本金5000，每期还款1767.65。APR是24%，IRR是36%。

![image-20241022213742373](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20241022213742373.png)



# 1.策略

规则和模型本质上都是策略的工具，只不过模型是复杂的规则（或者说规则是简单的模型）。

规则和模型都是为了业务策略服务的，规则和模型只是业务策略中得一个环节，只对自己得风险识别能力负责，至于要使用什么数据、阈值设定、调用顺序、使用位置等都是策略需要考虑得事情（特征变量由专门岗位的人产出，策略直接进入分析阶段）。

![image-20250105141824316](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105141824316.png)

## 1.1规则认识和类型

### 1.1.1策略维度

策略维度如下：评分类综合分中，目前消费分还没涉及到。

![image-20250105140839872](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105140839872.png)



### 1.1.2规则强弱程度

![image-20250105141003480](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105141003480.png)

弱规则一般是连续性数值变量，且变量**具有一定的排序性**。



### 1.1.3规则案例

![image-20250105141218246](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105141218246.png)

重点在：数据含义，规则描述，规则强度（案例中红色是强规则）

## 1.2.规则分析全流程介绍

### 1.2.1.规则全流程

规则完整流程：

1. 规则离线开发和评估
2. 规则上线和监控

![image-20250105144148183](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105144148183.png)



### 1.2.2.规则离线开发和评估

![image-20250105144336656](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105144336656.png)

1. 人工制定规则
   * 现有风险识别再制定规则目标，基于专家经验
2. 量化制定规则
   * 先做出衍生变量，再基于数据分析进行规则筛选

**实际业务中两者结合使用。**



**量化规则开发流程：**

通常情况下变量开发会提前用，用以快速分析上线，需要提前规划。 ![image-20250105144608612](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105144608612.png)



### 1.2.3.规则上线验证和监控

![image-20250105144857828](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105144857828.png)

1. **规则上线：**制定好的规则发布到决策引擎
2. **规则验证：**验证上线规则的配置、**AB测试分流比例是否正确**；==**上线后规则命中率与线下测算时命中率的误差**==
3. **规则监控：**对规则命中率的稳定性按周期（日/月/周）持续监控，如出现异常需及时排查。
4. **规则调整：**对已上线规则进行贷后分析，根据表现调整规则阈值。



## 1.3.基于单变量生成规则

单变量制定一般步骤：

![image-20250105145848936](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105145848936.png)



### 1.3.1 变量初筛

![image-20250105145935142](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105145935142.png)

变量初筛主要是对变量进行描述性统计分析（**如平均值、最大值、最小值、标准差等**）以及缺失率、众数占比等指标计算，**对不符合要求的变量先进行一轮剔除，这样可以减少后面二轮筛选规则的压力**。



==变量初筛过程中，还要注意一下特殊值，规则可能需要单独考虑特殊值：==

可以先看接口文档，了解大致的特殊值。

再通过代码看实际的分布，比如通过toad.detector来看：可以看到唯一值，标准差，各分位点的值等（众数占比需要自己算）

* 空值占比（null，空字符串等），以及空值的含义
* -999或者999，以及这些特殊值的含义
* 最小最大值等



众数占比高可能会导致无法分箱，没有排序性（连续性变量）。

### 1.3.2 变量分箱

![image-20250105151937779](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105151937779.png)

 主要关注分箱的注意事项：

1. 必须包括变量所有值，不能丢失信息
2. 分箱数量不宜太多，一般再5-8之间
3. 每箱数量占比至少在5%以上，数量太少没有统计意义
4. 如果有缺失值，需要单独分一箱（缺失值可能也有单独的含义）



**分箱算法分类：**

![image-20250105152305072](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105152305072.png)

 

### 1.3.3 分箱统计量

**分箱后统计量（好坏客户占比与坏账率）：**

![image-20250105153843493](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105153843493.png)

主要是**边际占比**与**区间占比**：

* **边际占比**：**分箱下的XX数量** / **XX总数量**（纵向对比）
* **区间占比**：**分箱下的XX数量** / **分箱内总客户数**。一般只关注坏客户，也叫区间坏账率



### 1.3.4 分箱统计量含义

**分箱后统计量的含义：**

![image-20250105154435504](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105154435504.png)

一般来说，规则和模型都要求变量有明显的单调性，这样符合业务的可解释性。



### 1.3.5 WOE-公式和含义

![image-20250105154753073](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105154753073.png)

WOE公式：

$$WOE_i = ln(\frac{该分箱内边际坏客户占比}{该分箱内边际好客户占比}) = ln(该分箱内边际坏客户占比)-ln(该分箱内边际好客户占比)=ln(\frac{Bad_i}{Bad_T}/\frac{Good_i}{Good_T})=ln(\frac{Bad_i}{Bad_T})-ln(\frac{Good_i}{Good_T})$$



### 1.3.6 IV-公式和含义

![image-20250105155503924](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105155503924.png)

分箱内好坏客户边际占比越大，IV越高，说明变量好坏区分度越明显。

### 1.3.7 IV-使用标准

![image-20250105160451166](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105160451166.png)

规则和模型变量筛选时是一样的：除IV外，也要考虑变量间的相关性。相关性高的变量==**综合考虑IV、变量的坏账排序性（业务可解释性）、规则效果LIFT**==等，选择性保留变量。



### 1.3.8 IV-代码实现

![image-20250105161041598](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105161041598.png)

如果三方包不满足使用要求，也可以自己写一个IV计算函数，但**前提是分箱、WOE也要自己提前处理好**。



### 1.3.9 制定规则阈值

#### 1.3.9.1 规则效果-两个期望

1. 拒绝客户占总体客户的比例（命中率）不宜过高

   * 命中率高影响业务规模
   * 每个环节维度不同，对风险识别互为补充。每个环节做最有把我的事情。
   * 由以上两点，规则阈值一般会比较极端，要的实现效果就是**通过大部分客户，而只拒绝一少部分客户，单规则一般拒绝比例不超过5%**

2. 拒绝客户中，坏客户占比越高越好，同时好客户占比越低越好

   尽可能抓坏，并减少误杀

#### 1.3.9.2 规则效果-5各评估项

前三个是可量化的指标，后两个是业务解释性相关。

![image-20250105163116028](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105163116028.png)

1. 精准率

   由于实际情况中，坏样本浓度比较低，很难达到较高的精准率（bad_rate），一般通过LIFT来比较（bad_rate / 整体坏样本率；或者）

2. 召回率

   精准率和召回率是需要同时兼顾的：**希望在精准率达到要求的情况下，尽可能提升召回率**。如果某条规则精准率达到80%，但是只拒绝了0.1%的坏客户这条规则是需要再考虑的（特殊情况，比如极端的反欺诈规则需要视实际情况而定）。

   精准率和召回率两个指标是互斥的。这个与通过率和坏账率平衡类似。 

#### 1.3.9.3 规则阈值制定方法-IV分析法

分箱后，直接拒绝某一个分箱的客群。这个和分箱方法有很大的关系。**这种方法实际上是按照分箱来拒绝**。

![image-20250105164225433](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105164225433.png)

#### 1.3.9.4 规则阈值制定方法-极端值检测

![image-20250105164855081](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250105164855081.png)

极端值检测通过“**枚举分位数**”的方式，**枚举可能的极端值，作为备选的阈值**。

分位数，也称分位点：是概率的概念。即在一个集合内，小于这个值的所有样本的概率。比如，1%分位点，小于这个值的样本的个数为整体数量的1%。

正经定义：分位数指的就是连续分布函数中的一个点，这个点对应概率p。若概率0<p<1，随机变量X或它的概率分布的分位数Za，是指满足条件p(X≤Za)=α的实数 [1]。

**通常使用的分位点如下：**(toad.detector也有分位点，但是只包含1%和99%，没有那么细)

[0.005, 0.01, 0.02, 0.05, 0.95, 0.98, 0.99, 0.995]：小于0.5%、1%、2%、5%；大于95%、98%、99%、99.5%

[0.5%, 1%, 2%, 5%, 95%, 98%, 99%, 99.5%]



## 1.4.基于交叉表生成规则

### 1.4.1 交叉表介绍

 

1. 交叉表本质是变量的笛卡尔积。![image-20250108211618678](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250108211618678.png)



2. 交叉表的复杂度介于单规则和评分卡模型之间。同时兼顾维度和复杂度两点。![image-20250108211813434](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250108211813434.png)



3. 交叉表的前置条件

   * **基于IV筛选除预测效果好的变量池，从中选择交叉所需的变量组**。一般原则是：交叉变量最好是不同维度的，且相关性不高，这样综合效果最优
   * **对变量进行分箱操作，连续型变量需要有排序性。**有排序性才有直观的可解释性。离散型变量很多时，也可以分箱。
   * **总样本和坏样本数量足够多**。总样本数不变，两个变量交叉后，样本会被稀释到各个格子内。如果总样本和坏客户数数量不够的话，会导致样本充分性不足，无统计意义。

   ![image-20250108211955245](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250108211955245.png)

   ![image-20250108212356902](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250108212356902.png)



### 1.4.2 交叉表规则生成和评估

交叉表制定步骤：

1. 基于透视表统计坏客户数和总客户数
2. 基于坏客户数和总客户数统计量，计算出区间坏账率（badrate）和客户数占比
3. 基于格子的区间坏账率和客户占比制定规则

![image-20250108212714841](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250108212714841.png)



**步骤一：**透视表

第一个透视表是坏客户数统计，第二个透视表是总客户数统计。使用pandas.crosstab+dataframe.pivot_table方法制作交叉表并统计数量；可以用seaborn热力图直观展示badrate

![image-20250108212950683](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250108212950683.png)



**步骤二：**计算区间坏账率（badrate）和区间客户占比。

![image-20250108213130627](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250108213130627.png)



**步骤三：**基于格子的区间坏账率和客户占比制定规则。（平衡通过率和逾期率） 。

评估精准率、召回率、通过率 

![image-20250108213244674](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250108213244674.png)



应用场景：D类调优（收紧风险，但不太影响通过率）

![image-20250108213903570](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250108213903570.png)



### 1.4.3 如何寻找变量

分为离散型变量与连续型变量两类进行探索(**本质上还是根据IV和排序性挑选**，此外还要考虑变量间的相关性)

可以通过pandas自动判断类型来区分连续型和离散型变量，object一般来说都是离散型变量。可以用`DataFrame.select_dtypes(include=None, exclude=None)`挑选离散型变量。

toad对离散型变量也能分箱（默认，

1. 离散型变量
   * 剔除掉离散值过多的变量。做交叉表类别太多。
   * 观察各离散值的badrate，挑选出排序性较好的离散变量。
2. 连续型变量
   * 分箱后根据IV和排序性挑选。



## 1.5.基于决策树生成规则

### 1.5.1 什么是决策树

![image-20250120203056388](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120203056388.png)

### 1.5.2 决策树的生成过程

决策树一般生成过程，相当于通过特征变量对样本的特征空间做**非线性划分**，所以也说决策树是“**非线性**”模型。

![image-20250120203210868](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120203210868.png)

### 1.5.3 决策树算法

![image-20250120203513701](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120203513701.png)



### 1.5.4 CART决策树

![image-20250120203559848](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120203559848.png)

二分类中：p=1/2时（无法区分），基尼系数最大，此时效果最差。

### 1.5.5 CART分类树-基尼系数

![image-20250120204149378](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120204149378.png)  

  

![image-20250120205007624](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120205007624.png)

基尼指数：反映从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据集D的纯度越高。



### 1.5.6 CART分类树-变量二分法

连续型变量：

![image-20250120205212382](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120205212382.png)  



离散型变量：

![image-20250120205518066](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120205518066.png)



### 1.5.7 CART分类树-递归生成

![image-20250120205857518](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120205857518.png)

### 1.5.8 CART回归树-预测方式

![image-20250120210741848](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120210741848.png)

![image-20250120212201151](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120212201151.png)

与回归的度量标准也不同，类似损失函数。

回归树也有分类的思想（聚类）。相同类之间的目标变量值才会更接近，方差才会更小 。

* 分类树：使用基尼系数切分。
* 回归树：使用残差平方和（RSS）切分。

### 1.5.9 CART回归与分类树的预测方式

* 分类树：叶子节点中概率最大的类别作为当前节点的预测类别。
* 回归树：叶子节点中样本的y均值作为回归的预测值。



==CART决策树还有剪枝的过程，课程中不涉及需要另外看资料==

### 1.5.10 决策树规则生成与调参

**调参：（决策树及调参代码见[1.5.11](#s1)）**

1. **改变决策树算法本身的参数**。比如叶子节点样本数量（比例），树深度等等。比如**控制树深度，可以大致控制叶子节点是单规则（1层），二维交叉规则（2层）还是多维组合规则（多层）**。树深度不完全决定组合规则的数量，只决定上限（可能会提前分裂结束）。
2. **改变入模变量的组合**（python-combinations方法）。不同的入模变量组合（**不同子集的基尼系数不同**），决策树会有不同的分裂过程，得到不同的规则组合方式。



![image-20250120212705572](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120212705572.png)

**注意事项：**

* 过拟合：规则复杂可能在训练样本上效果好，OOT上效果差。
* 树深度：树深度过深可能会导致规则过于复杂，不利于上线后监控维护。==**一般规则包含的变量不超过三个**==
* ==**调参**==：

![image-20250120212857741](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120212857741.png)



### 1.5.11决策树Python代码

主要依靠sklearn的API。

sklearn只实现了ID3和C4.5，只实现了CART算法，并且是调优过的。（**暂不支持类别型特征**）

![image-20250120213121965](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120213121965.png)

![image-20250120213344758](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120213344758.png)

![image-20250120213450214](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250120213450214.png)

<a name="s1"></a>

通过调整入模变量组合，以及获得组合路径的代码，在这三节课中：

1. **基于单变量生成规则（2）：Python代码实操**。决策树可视化，以及展示参数调整。

2. **基于决策树生成规则(3)：自动化挖掘**。决策树分裂路径记录（组合规则）（深度优先遍历递归）

3. **并行规则集性能测试(2)：Python实操**。针对不同的入模变量组合，循环训练得到组合规则结果。这部分没有直接的代码，实际上是复用第二节课里的核心代码（决策树路径记录）。

   1. 主要代码如下（得到变量组合）：

      ![image-20250218215110743](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250218215110743.png)

   2. 对不同入模变量组合进行决策树训练

      在单组入模变量训练的基础上，套了个循环，把所有入模变量的组合都跑一边，最后把结果存到一起。核心函数还是第二个课里的**extract_tree_rules（）**方法。

      1. 变量r是所有变量的个数。
      2. 单变量规则，树的深度定为1，组合子集个数为2。**入模变量两两组合，得到单变量规则**。
      3. 多变量组合规则，树的深度定为2及以上，组合个数为所有变量个数（**如果选择将组合数减少，那么会多出非常多的组合，得到很多的组合变量**）。

      ![image-20250218215419551](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250218215419551.png)



## 1.6.规则泛化效果评估

OOT和训练集一般是在开始的时候就一起设计好，然后特征变量一次性全部加工好，提高效率。

**选取OOT的时候，最好还要考虑一下OOT与训练集时间段内，有没有做过规则的调整（可能会导致客群有差别）**。



### 1.6.1 样本选择的几个注意点：

* OOT时间窗口在训练集之后。
* OOT**一般选3个月以上**的样本，并按月分别评估（为满足统计意义上的样本数量最小要求）
* OOT需要和训练集一样，具有一定的贷后表现时间（**OOT在没有完全表现的情况下，建议最少也得有3个月的表现**）
* OOT的客群的风险水平与训练集相同（**部分原因：比如中间有过策略调整，有可能会出现OOT客群与训练集出现不同，可能会导致规则效果不稳定**）。简单衡量风险水平的方法：可以观察训练集和OOT的平均坏样本率和相同特征下的分箱分布情况。

![image-20250211210354463](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250211210354463.png)



![image-20250211210851777](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250211210851777.png)



### 1.6.2 评估指标

**逾期率评估指标：（训练集和OOT都可以看）**

* 命中率
* 精准率（precision，badrate）
* 召回率
* lift
* ==逾期率下降幅度（除绝对值外，还可以看相对值）==：（样本总的坏账率-规则通过后样本坏账率）/样本总坏账率



**指标浮动范围：**

1. 如果时间外样本上的指标在分析样本指标的一定范围内浮动，则说明泛化样本上效果打标，如果超出范围下降严重则说明不达标浮动范围一般不超过10%，严格的不超过5%。
2. 比如，分析样本lift为3，时间外样本lift为1，效果衰减较多（说明规则可能过拟合），需要重新调整规则；如果时间外样本lift为2.7-3.3之间，不超过10%的范围内波动，则说明规则的泛化能力达标。



**通过率评估：**

精准率，召回率，lift等指标需要有Y标签，导致通过率可能不太准确。**可以采用最新的样本来评估规则通过率影响，最新样本的命中率也可以和训练集和OOT上的命中率做对比，如果波动太大，需要考虑使用**。

![image-20250211211911379](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250211211911379.png)

## 1.7 规则集线下性能测试

规则集主要有三个指标：

1. 综合命中率
2. 整体坏账率（规则集命中的客户中的坏客户比例）
3. 相互覆盖率（规则与规则之间覆盖的程度，是**独立贡献**的指标。如果一个规则完全被其它规则覆盖，那么该规则在规则集中就没有存在价值了，统计**单一命中率**） 

![image-20250218203830100](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250218203830100.png)



![image-20250218204412075](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250218204412075.png)



如果要剔除规则1，那么规则1剔除后需要重新计算规则2和3的单一命中率（**先剔除无效规则，再统一计算单一命中率**）

![image-20250218204653685](%E9%A3%8E%E6%8E%A7%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20250218204653685.png)



