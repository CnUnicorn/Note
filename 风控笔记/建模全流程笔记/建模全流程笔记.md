[toc]



# 1、数据处理

## 1.1、读取数据，客户打标

1. 读取数据

2. 通过滚动率或迁徙率，**确定坏客户定义**（一般是定义为逾期M1+为坏客户-逾期30天以上）。如果逾期天数无法确定坏客户（如某些助联贷产品无客户真实还款记录），可以考虑用其它口径定义坏客户。

3. **通过Vintage确定表现期**，随着账龄增加，逾期率变化逐渐平缓，选取切点作为表现期（最完整的表现期=产品最大周期）。确定好表现期后，就可以确定样本客户的时间范围。

4. 客户打标，需要从业务维度转换成客户维度。当一个客户存在多笔业务记录时：

   这里的逾期天数根据第二步的迁徙率和滚动率来确定，一般是30天。

   1. 首先对每笔业务打标。
      * 好业务：最大历史逾期天数=0
      * 坏业务：最大历史逾期天数>30
      * 灰度业务：最大历史逾期天数 in (0, 30] 
   2. 业务维度转换成客户维度。
      * 好客户：名下所有业务，都是好业务
      * 坏客户：名下存在坏业务
      * 灰度客户：名下不存在坏业务，但存在灰度业务

5. 灰度样本处理：一般是剔除。如果灰度样本较多，也可以考虑将灰度样本添加进好样本中。

6. 数据集切分，按7:3切分训练集与测试集，或者采用K折交叉验证的方式充分利用数据集。



## 1.2、数据预处理

### 1.2.1、异常值与缺失值处理

1. 均方差法

   在统计学中，如果一个数据分布近似正态，那么大约 68% 的数据值会在均值的一个标准差范围内，大约 95% 会在两个标准差范围内，大约 99.7% 会在三个标准差范围内。

   可以以均值为起点，左右三个标准差作为cut-off，剔除这个范围外的数据。

   **不过实际金融数据中，不一定所有的特征都是正态分布的，所以需要慎重使用。**

![已上传的图片](%E5%BB%BA%E6%A8%A1%E5%85%A8%E6%B5%81%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/file-4JaxLTpzU4oeB6Y26jeZZKj4)

2. 基于聚类的方法

   基于聚类的方法可以用聚类算法（如k-means、DBSCAN 等）将训练样本分成若干类，如果某一个类中的样本数很少，而且类中心和其他所有类的距离都很远，那么这个类中的样本极有可能是异常特征样本。

3. 专门的异常点监测算法

   孤立森林（Isolation Forest）是一种应用广泛的异常点检测 算法。在孤立森林中，异常被定义为“容易被孤立的离群点”（more likely to be separated）， 我们可以将其理解为分布稀疏且与密度高的群体较远的点。在特征空间中，分布稀疏的区域表示事件发生在该区域的概率很低，因而我们可以认为落在这些区域里的数据是异常的。

4. 异常值处理

   **异常值需要谨慎处理，风控数据中的异常值也可能是反欺诈行为产生的，需要从业务含义再次确认。**

   （1）选择直接删除异常值样本

   （2）结合特征含义选择置空异常值，或者填充为其他值

   （3）两类无法通过技术手段检测出的特征：周期性变化的特征，影响稳定性，需要剔除；具备明显缺陷的特征，如有些数据后续埋点不会再有。

### 1.2.2、缺失值处理

1. 非正常缺失

   由于数据存储，数据接口异常等问题导致的特征缺失。最好是修复相关数据后再回溯；如果不可修复，就将特征标记成-9999或-8888等特殊含义的值。

2. 正常缺失

   例如，对于客户选择授权的数据，部分客户接受，部分客户拒绝；或者某些比率值，分母为0。可能会导致后续计算特征缺失，这些情况需要在建模种结合业务含义再确认。

   （1）一般线性回归模型中的缺失值，如果需要填充，根据业务含义采取均值、众数、中位数等填充；或者缺失值单独作为一箱

   （2）在使用决策树建模时，比如XGBoost，算法会自动处理缺失值

### 1.2.3、区分类别型变量与连续型变量

<img src="%E5%BB%BA%E6%A8%A1%E5%85%A8%E6%B5%81%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20240709210906888.png" alt="image-20240709210906888" style="zoom: 50%;" />



1. 类别型变量与连续型变量的区分

   * 一般来说非数值型变量都可以当成类别型变量，比如非int，float类型。基本可以认定为类别型变量。

   * 数值型变量又可以分为连续型变量与类别型变量。比如，某些特征变量是码值，这个特征去重后不超过10种，那么可以认定为类别型变量（种类阈值可以调整，可以是10种，也可以是20种，根据特征具体含义确定）

   * 一般情况下：**非数值型变量** -> 类别型变量；**数值型变量** -> 类别型数值变量 + 非类别型数值变量

     

2. 连续型变量与类别型变量的处理

   * 数值型变量，一般分箱后进行WOE编码（数值离散化）转换，再用于线性回归计算。个人理解，分箱后数据映射到分箱，实际上也变成了类别型变量，再通过WOE编码将数据分布计算成数值入模

   * 类别型变量

     * 小于5种时，可以考虑直接编码后入模（如某个特征变量类别较少，直接一个类别一个分箱进行WOE编码）；大于5种时，建议做降维处理后，再根据降维后的类别做分箱

     * 一般需要先编码后（类别型特征数值化），再入模。常见的编码方式有：**序数编码（有明显级别含义的）**、**one-hot编码（维度低的纯分类特征，单纯标记类别无含义）**、**目标编码**、**WOE**编码。

       如果是逻辑回归建模，特征编码后，接着分箱，计算WOE；如果是一类一箱，也可以直接计算WOE

     * 分箱后，连续型数值变量也变成了类别型变量，转换成WOE编码。



# 2、数据分箱

特征分箱，toad中的几种分箱方式：等频('quantile')、等距('step')、卡方('chi')、决策树('dt')、’kmeans‘

- 特征分箱的目的：
  - 从模型效果上来看，特征分箱主要是为了降低变量的复杂性，减少变量噪音对模型的影响，提高自变量和因变量的相关度。从而使模型更加稳定。
- 数据分桶的对象：
  - 将连续变量离散化
  - 将多状态的离散变量合并成少状态
- 分箱的原因：
  - 数据的特征内的值跨度可能比较大，对有监督和无监督中如k-均值聚类它使用欧氏距离作为相似度函数来测量数据点之间的相似度。都会造成大吃小的影响，其中一种解决方法是对计数值进行区间量化即数据分桶也叫做数据分箱，然后使用量化后的结果。
- 分箱的优点：
  - 处理缺失值：当数据源可能存在缺失值，此时可以把null单独作为一个分箱。
  - 处理异常值：当数据中存在离群点时，可以把其通过分箱离散化处理，从而提高变量的鲁棒性（抗干扰能力）。例如，age若出现200这种异常值，可分入“age > 60”这个分箱里，排除影响。
  - 业务解释性：我们习惯于线性判断变量的作用，当x越来越大，y就越来越大。但实际x与y之间经常存在着非线性关系，**此时可经过WOE变换**。
- 特别要注意一下分箱的基本原则：
  - （1）最小分箱占比不低于5%
  - （2）箱内不能全部是好客户
  - （3）连续箱单调



# 3、特征筛选

在数据预处理、以及分箱完成的情况下：

通过区分度，稳定性，相关性几个方面进行特征筛选

<img src="%E5%BB%BA%E6%A8%A1%E5%85%A8%E6%B5%81%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20240709210352449.png" alt="image-20240709210352449" style="zoom: 50%;" />

## 3.1、剔除IV较小的特征

一般是剔除IV < 0.02的特征。

IV与分箱方法有关（决定了数据分布），IV公式如下：

每一箱的坏客户与总体坏客户的比例，与每一箱的好客户与总体好客户的比例，之间的分布关系

![image-20240709211552611](%E5%BB%BA%E6%A8%A1%E5%85%A8%E6%B5%81%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20240709211552611.png)

![image-20240709212338745](%E5%BB%BA%E6%A8%A1%E5%85%A8%E6%B5%81%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20240709212338745.png)

## 3.2、相关性

**相关性：**

一般选取相关性 < 0.65 或者选择 相关性 < 0.7的特征对中，IV较大的那一个。

实际操作过程中，可以两个都试试。



**P值：**

相关性满足要求后，还要看这对变量的P值。一般认为P值大于0.05时，这对变量计算得到的相关性不显著，说明这个相关性结果可能是由于某些随机的原因得到。这个时候也考虑在两者中选IV较大的那个（实际操作过程，也可以两个都试试）



## 3.3、逐步回归

利用逐步回归剔除多变量共线性，以及剔除多变量中解释性较弱的那些。



## 3.4、VIF校验

利用VIF参数，消除多变量的多重共线性。一般认为VIF > 10，表明这个变量存在多重共线性的情况，需要剔除。



## 3.5、PSI

PSI用于衡量变量的稳定性，公式如下：

![image-20240709212424819](%E5%BB%BA%E6%A8%A1%E5%85%A8%E6%B5%81%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20240709212424819.png)

![image-20240709212434468](%E5%BB%BA%E6%A8%A1%E5%85%A8%E6%B5%81%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20240709212434468.png)

PSI计算公式与IV类似，本质都是KL散度（什么是KL散度？）



## 3.6、特征降维

特征降维有PCA、LDA、LLE、MDS等方法。

降维后会丢失信息，且解释性没有原特征强

### 3.6.1、线性提取

1. PCA

   PCA 将高维的特征向量合并为低维的特征向量，是一种无监督的特征提取方法。PCA 通过线性投影，将高维数据映射到低维空间中表示，并且期望在所投影的维度上数据的方差最大（最大方差理论），以此使用较少的数据维度，留存较多的原数据特性。

   ```python
   '''
   sklearn 库的 decomposition 包中的 PCA 类可以实现利用 PCA 进行特征提取
   n_components 参数用来控制降维程度。当 n_components 取小数时，如 0.9，表示保留原始
   数据集 90%的信息；当 n_components 取整数时，表示将原始数据降到多少维。
   '''
   # sklearndecomposition 包中的 PCA 类可以实现利用 PCA 进行特征提取
   from sklearn.decomposition import PCA
   pca = PCA(n_components=0.9)
   train_x_new = pca.fit_transform(train_x)
   ```

   

2. LDA

   LDA 是一种基于分类模型进行特征属性合并的操作，是一种有监督的特征提取方法。LDA 的原理是，将带标签的数据投影到维度更低的空间中，使得投影后的点按类别区分，相同类 别的点会在投影后的空间中更接近，用一句话概括：投影后相同类间方差最小，不同类间方 差最大。

   ```python
   '''
   sklearn 库的 discriminant_analysis 包中的 LinearDiscriminantAnalysis 类可以方便地利用 LDA 进
   行特征提取，其中 n_components 参数用来指定降到的维数，且取值只能为[1,标签类别数-1)区间中的整数
   '''
   #sklearn 库可以实现利用 LDA 进行特征提取
   from slearn.discriminant_analysis import LinearDiscriminantAnalysis
   ida = LinearDiscriminantAnalysis(n_components=5)
   train_x_new = lda.fit_transform(train_x, train_y)
   ```

   ### 3.6.2、非线性提取

   LLE、MDS



# 5、模型效果评价

交叉熵（BCE），相比准确率（accuracy）更普适。



# 读书笔记

1、可以将缺失值，填充到坏客率相近的分箱中，使得分箱单调
2、除坏客率单调外，还需要看不同时间段的分箱的变化。如果有交叉，需要将交叉的分箱合并
3、分数映射，可以从逻辑回归推导出，每个特征的分数映射公式（对应toad源码scorecard中woe_to_score方法）
4、模型映射可以通过动态调整pdo，使得不同区间范围内的客户分布保持不变

<img src="%E5%BB%BA%E6%A8%A1%E5%85%A8%E6%B5%81%E7%A8%8B%E7%AC%94%E8%AE%B0.assets/image-20240826231245445.png" alt="image-20240826231245445" style="zoom:50%;" />